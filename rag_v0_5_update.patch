*** Begin Patch
*** Update File: server/app/embeddings.py
@@
-# File: server/app/embeddings.py
-# Project: RAG_project_v0.5  Component: embeddings  Version: v0.9.2
-from __future__ import annotations
-
-import hashlib
-import inspect
-import math
-import os
-import time
-from typing import Dict, Iterable, List, Optional, Sequence, Tuple
-
-import httpx
-import numpy as np
-
-try:
-    from .embed_cache import EmbedCache  # optional, adapter handles variants
-except Exception:
-    EmbedCache = None  # type: ignore
-
-
-MODEL = os.getenv("MODEL", "text-embedding-3-small")
-DIM_DEFAULT = int(os.getenv("EMBED_DIM", "1536"))
-ALLOW_REMOTE = os.getenv("ALLOW_REMOTE_EMBEDDINGS", "1") == "1"
-OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
-OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
-OPENAI_TIMEOUT_SECS = float(os.getenv("OPENAI_TIMEOUT_SECS", "30"))
-MAX_RETRIES = int(os.getenv("EMBED_MAX_RETRIES", "4"))
-BACKOFF_MIN = float(os.getenv("EMBED_BACKOFF_MIN_MS", "500")) / 1000.0
-BACKOFF_MAX = float(os.getenv("EMBED_BACKOFF_MAX_MS", "4000")) / 1000.0
-
-_singleton_fake: Optional["FakeEmbedder"] = None
-_singleton_remote: Optional["OpenAIEmbedder"] = None
-_cache_singleton: Optional["EmbedCache"] = None
-
-
-def _get_cache() -> Optional["EmbedCache"]:
-    global _cache_singleton
-    if _cache_singleton is not None:
-        return _cache_singleton
-    if EmbedCache is None:
-        return None
-    try:
-        _cache_singleton = EmbedCache()  # type: ignore[call-arg]
-        return _cache_singleton
-    except Exception:
-        return None
-
-
-def _hash_text(text: str) -> str:
-    return hashlib.sha256(text.encode("utf-8")).hexdigest()
-
-
-def _l2_normalize(vec: Sequence[float]) -> List[float]:
-    s = math.sqrt(sum((x * x) for x in vec)) or 1.0
-    return [float(x / s) for x in vec]
-
-
-class _CacheAdapter:
-    def __init__(self, cache: EmbedCache):
-        self.cache = cache
-        self.sig_get = None
-        self.sig_put = None
-        if hasattr(cache, "get_many"):
-            self.sig_get = inspect.signature(cache.get_many)  # type: ignore
-        if hasattr(cache, "put_many"):
-            self.sig_put = inspect.signature(cache.put_many)  # type: ignore
-
-    def get_many(self, pairs: Iterable[Tuple[str, str]]) -> Dict[str, List[float]]:
-        if not self.sig_get:
-            return {}
-        params = list(self.sig_get.parameters.values())
-        if len(params) == 2:
-            return self.cache.get_many(list(pairs))  # type: ignore
-        if len(params) == 4:
-            hashes = [h for (h, _m) in pairs]
-            model = next(iter(pairs))[1] if pairs else MODEL
-            dim = DIM_DEFAULT
-            return self.cache.get_many(hashes, model, dim)  # type: ignore
-        return {}
-
-    def put_many(
-        self, tuples: Iterable[Tuple[str, str, int, List[float]]]
-    ) -> None:
-        if not self.sig_put:
-            return
-        params = list(self.sig_put.parameters.values())
-        if len(params) == 2:
-            self.cache.put_many(list(tuples))  # type: ignore
-            return
-        if len(params) == 5:
-            hashes = [h for (h, _m, _d, _v) in tuples]
-            model = next(iter(tuples))[1] if tuples else MODEL
-            dim = next(iter(tuples))[2] if tuples else DIM_DEFAULT
-            vecs = [v for (_h, _m, _d, v) in tuples]
-            self.cache.put_many(hashes, model, dim, vecs)  # type: ignore
-
-
-class FakeEmbedder:
-    def __init__(self, dim: int) -> None:
-        self.dim = int(dim)
-
-    def embed_texts(self, texts: List[str]) -> List[List[float]]:
-        out: List[List[float]] = []
-        for t in texts:
-            h = hashlib.blake2b(t.encode("utf-8"), digest_size=32).digest()
-            rng = np.random.default_rng(int.from_bytes(h[:8], "big"))
-            vec = rng.standard_normal(self.dim).astype(np.float32)
-            out.append(_l2_normalize(vec.tolist()))
-        return out
-
-
-class OpenAIEmbedder:
-    def __init__(self, model: str, dim: int, timeout_s: float) -> None:
-        self.model = model
-        self.dim = int(dim)
-        self.timeout_s = float(timeout_s)
-
-    def _client(self) -> httpx.Client:
-        return httpx.Client(
-            base_url=OPENAI_BASE_URL,
-            timeout=self.timeout_s,
-            headers={"Authorization": f"Bearer {OPENAI_API_KEY}"},
-        )
-
-    def embed_texts(self, texts: List[str]) -> List[List[float]]:
-        if not texts:
-            return []
-        cache = _get_cache()
-        adapter = _CacheAdapter(cache) if cache else None
-
-        out: Dict[int, List[float]] = {}
-        to_query: List[Tuple[int, str]] = []
-
-        hashes = [_hash_text(t) for t in texts]
-        if adapter:
-            got = adapter.get_many([(h, self.model) for h in hashes])
-            for i, h in enumerate(hashes):
-                v = got.get(h)
-                if v and len(v) == self.dim:
-                    out[i] = v
-                else:
-                    to_query.append((i, texts[i]))
-        else:
-            to_query = list(enumerate(texts))
-
-        if to_query:
-            ordered = [t for (_i, t) in to_query]
-            backoff = BACKOFF_MIN
-            for attempt in range(MAX_RETRIES):
-                try:
-                    with self._client() as cli:
-                        resp = cli.post(
-                            "/embeddings",
-                            json={"model": self.model, "input": ordered},
-                        )
-                    resp.raise_for_status()
-                    data = resp.json()
-                    vecs = [d["embedding"] for d in data.get("data", [])]
-                    if adapter:
-                        tuples = [
-                            (_hash_text(t), self.model, self.dim, _l2_normalize(v))
-                            for t, v in zip(ordered, vecs)
-                        ]
-                        adapter.put_many(tuples)
-                    for (i, _), v in zip(to_query, vecs):
-                        out[i] = _l2_normalize(v)
-                    break
-                except Exception:
-                    if attempt + 1 >= MAX_RETRIES:
-                        raise
-                    time.sleep(min(backoff, BACKOFF_MAX))
-                    backoff *= 2.0
-
-        return [out[i] for i in range(len(texts))]
-
-
-def get_embedder(dim_hint: Optional[int] = None, force_local: bool = False):
-    dim = int(dim_hint or DIM_DEFAULT or 1536)
-    if not force_local and ALLOW_REMOTE:
-        global _singleton_remote
-        if _singleton_remote is None or _singleton_remote.dim != dim:
-            _singleton_remote = OpenAIEmbedder(MODEL, dim, OPENAI_TIMEOUT_SECS)
-        return _singleton_remote
-    global _singleton_fake
-    if _singleton_fake is None or _singleton_fake.dim != dim:
-        _singleton_fake = FakeEmbedder(dim)
-    return _singleton_fake
-
-
-__all__ = ["FakeEmbedder", "OpenAIEmbedder", "get_embedder"]
+"""
+File: server/app/embeddings.py
+Project: RAG_project_v0.5  Component: embeddings  Version: v1.3.0
+
+Enhancements:
+ - Uniform logging via LOG_LEVEL
+ - Top-level embed_texts() convenience used by code_ingest.py
+ - Live backpressure (RPM/TPM with headroom) + simple EMA timing
+ - Optional SQLite/LRU cache adapter (EmbedCache) compatibility
+ - Dimension guard: enforces target dim; RP reduction when needed
+ - Debug snapshot for /v1/debug/embedding-status
+"""
+from __future__ import annotations
+
+import hashlib
+import inspect
+import logging
+import math
+import os
+import time
+from typing import Dict, Iterable, List, Optional, Sequence, Tuple
+
+import httpx
+import numpy as np
+
+try:
+    from .embed_cache import EmbedCache  # optional
+except Exception:  # pragma: no cover
+    EmbedCache = None  # type: ignore
+
+# ------------------------ Config / ENV ------------------------
+LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO").upper()
+MODEL = os.getenv("MODEL", os.getenv("EMBEDDING_MODEL", "text-embedding-3-small"))
+DIM_DEFAULT = int(os.getenv("EMBED_DIM", "1536"))
+ALLOW_REMOTE = os.getenv("ALLOW_REMOTE_EMBEDDINGS", "1").strip().lower() in ("1", "true", "yes", "on")
+OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
+OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
+OPENAI_TIMEOUT_SECS = float(os.getenv("OPENAI_TIMEOUT_SECS", "30"))
+MAX_RETRIES = int(os.getenv("EMBED_MAX_RETRIES", "4"))
+BACKOFF_MIN = float(os.getenv("EMBED_BACKOFF_MIN_MS", "500")) / 1000.0
+BACKOFF_MAX = float(os.getenv("EMBED_BACKOFF_MAX_MS", "4000")) / 1000.0
+
+# Backpressure
+WINDOW_SEC = float(os.getenv("EMBED_WINDOW_SEC", "60"))
+CONCURRENCY = int(os.getenv("EMBED_CONCURRENCY", "4"))  # advertised to debug route
+RPM_LIMIT = int(os.getenv("EMBED_RPM_LIMIT", "5000"))
+TPM_LIMIT = int(os.getenv("EMBED_TPM_LIMIT", "1000000"))
+HEADROOM = float(os.getenv("EMBED_HEADROOM", "0.90"))
+
+log = logging.getLogger("embeddings")
+if not log.handlers:
+    logging.basicConfig(
+        level=getattr(logging, LOG_LEVEL, logging.INFO),
+        format="%(asctime)s %(levelname)-7s embeddings %(message)s",
+    )
+log.setLevel(getattr(logging, LOG_LEVEL, logging.INFO))
+
+# ------------------------ Helpers ------------------------
+def _hash_text(text: str) -> str:
+    return hashlib.sha256((text or "").encode("utf-8")).hexdigest()
+
+
+def _approx_tokens(s: str) -> int:
+    return (len(s) + 3) // 4 if s else 0
+
+
+def _l2_normalize(vec: Sequence[float]) -> List[float]:
+    s = math.sqrt(sum((x * x) for x in vec)) or 1.0
+    return [float(x / s) for x in vec]
+
+
+# ------------------------ Cache adapter ------------------------
+class _CacheAdapter:
+    def __init__(self, cache: EmbedCache):
+        self.cache = cache
+        self.sig_get = getattr(cache, "get_many", None) and inspect.signature(cache.get_many)  # type: ignore
+        self.sig_put = getattr(cache, "put_many", None) and inspect.signature(cache.put_many)  # type: ignore
+
+    def get_many(self, pairs: Iterable[Tuple[str, str]]) -> Dict[str, List[float]]:
+        if not self.sig_get:
+            return {}
+        params = list(self.sig_get.parameters.values())
+        if len(params) == 2:
+            return self.cache.get_many(list(pairs))  # type: ignore
+        if len(params) == 4:
+            hashes = [h for (h, _m) in pairs]
+            model = next(iter(pairs))[1] if pairs else MODEL
+            dim = DIM_DEFAULT
+            return self.cache.get_many(hashes, model, dim)  # type: ignore
+        return {}
+
+    def put_many(self, tuples: Iterable[Tuple[str, str, int, List[float]]]) -> None:
+        if not self.sig_put:
+            return
+        params = list(self.sig_put.parameters.values())
+        if len(params) == 2:
+            self.cache.put_many(list(tuples))  # type: ignore
+            return
+        if len(params) == 5:
+            hashes = [h for (h, _m, _d, _v) in tuples]
+            model = next(iter(tuples))[1] if tuples else MODEL
+            dim = next(iter(tuples))[2] if tuples else DIM_DEFAULT
+            vecs = [v for (_h, _m, _d, v) in tuples]
+            self.cache.put_many(hashes, model, dim, vecs)  # type: ignore
+
+
+def _get_cache() -> Optional["EmbedCache"]:
+    if EmbedCache is None:
+        return None
+    try:
+        return EmbedCache()  # type: ignore[call-arg]
+    except Exception:
+        return None
+
+
+# ------------------------ Limiters ------------------------
+class _Limiter:
+    """Simple windowed limiter with headroom; tracks rpm/tpm and exposes a snapshot."""
+
+    def __init__(self, window_sec: float, rpm: int, tpm: int, headroom: float):
+        self.window_sec = max(1.0, float(window_sec))
+        self.rpm = max(0, int(rpm))
+        self.tpm = max(0, int(tpm))
+        self.headroom = max(0.1, min(1.0, float(headroom)))
+        self._start = time.monotonic()
+        self._req = 0
+        self._tok = 0
+        self.cooldown_until: float = 0.0
+
+    def _maybe_roll(self) -> None:
+        if time.monotonic() - self._start >= self.window_sec:
+            self._start = time.monotonic()
+            self._req = 0
+            self._tok = 0
+            self.cooldown_until = 0.0
+
+    def account(self, tokens: int, requests: int) -> None:
+        self._maybe_roll()
+        self._tok += max(0, int(tokens))
+        self._req += max(0, int(requests))
+
+    def maybe_sleep(self) -> float:
+        self._maybe_roll()
+        need_sleep = False
+        if self.rpm > 0 and self._req > self.rpm * self.headroom:
+            need_sleep = True
+        if self.tpm > 0 and self._tok > self.tpm * self.headroom:
+            need_sleep = True
+        if not need_sleep:
+            return 0.0
+        end = self._start + self.window_sec
+        now = time.monotonic()
+        sleep_s = max(0.0, end - now)
+        if sleep_s > 0:
+            self.cooldown_until = end
+            log.info("[EMB][PACE] sleeping %.2fs (rpm=%d/%d, tpm=%d/%d, head=%.2f)",
+                     sleep_s, self._req, self.rpm, self._tok, self.tpm, self.headroom)
+            time.sleep(sleep_s)
+        self._maybe_roll()
+        return sleep_s
+
+    def snapshot(self) -> Dict[str, float | int]:
+        self._maybe_roll()
+        return {
+            "rpm_used": self._req,
+            "tpm_used": self._tok,
+            "rpm_budget": self.rpm,
+            "tpm_budget": self.tpm,
+            "headroom": float(self.headroom),
+            "cooldown_until": self.cooldown_until or 0.0,
+        }
+
+
+# ------------------------ Embedders ------------------------
+class FakeEmbedder:
+    def __init__(self, dim: int) -> None:
+        self.dim = int(dim)
+
+    def embed_texts(self, texts: List[str]) -> List[List[float]]:
+        out: List[List[float]] = []
+        for t in texts:
+            h = hashlib.blake2b((t or "").encode("utf-8"), digest_size=32).digest()
+            rng = np.random.default_rng(int.from_bytes(h[:8], "big"))
+            vec = rng.standard_normal(self.dim).astype(np.float32).tolist()
+            out.append(_l2_normalize(vec))
+        return out
+
+    # For parity with OpenAIEmbedder
+    def debug_snapshot(self) -> Dict[str, float | int | None]:
+        return {
+            "rpm_used": 0,
+            "tpm_used": 0,
+            "rpm_budget": 0,
+            "tpm_budget": 0,
+            "headroom": 1.0,
+            "cooldown_until": 0.0,
+            "inflight_without_ttfb": 0,
+            "ema_oneway_ms": 0.0,
+        }
+
+
+class OpenAIEmbedder:
+    def __init__(self, model: str, dim: int, timeout_s: float) -> None:
+        self.model = model
+        self.dim = int(dim)
+        self.timeout_s = float(timeout_s)
+        self._limiter = _Limiter(WINDOW_SEC, RPM_LIMIT, TPM_LIMIT, HEADROOM)
+        self._ema_ms: float = 0.0  # exponential moving average of 1-way latency
+
+    def _client(self) -> httpx.Client:
+        return httpx.Client(
+            base_url=OPENAI_BASE_URL,
+            timeout=self.timeout_s,
+            headers={"Authorization": f"Bearer {OPENAI_API_KEY}"},
+        )
+
+    def _enforce_dim(self, v: Sequence[float]) -> List[float]:
+        """Ensure vector matches self.dim; reduce/truncate/pad as needed."""
+        d = len(v)
+        if d == self.dim:
+            return _l2_normalize(v)
+        # Common case: 3072 -> 1536 via RP reducer
+        if d == 3072 and self.dim == 1536:
+            try:
+                from .reduction import get_reducer
+                red = get_reducer()
+                v2 = red.reduce(v)
+                return _l2_normalize(v2)
+            except Exception as e:  # pragma: no cover
+                log.warning("Reducer unavailable; truncating 3072->1536: %s", e)
+                return _l2_normalize(list(v)[:1536])
+        if d > self.dim:
+            log.warning("Embedding dim %d > target %d — truncating", d, self.dim)
+            return _l2_normalize(list(v)[: self.dim])
+        if d < self.dim:
+            log.warning("Embedding dim %d < target %d — zero-padding", d, self.dim)
+            pad = [0.0] * (self.dim - d)
+            return _l2_normalize(list(v) + pad)
+        return _l2_normalize(v)
+
+    def embed_texts(self, texts: List[str]) -> List[List[float]]:
+        if not texts:
+            return []
+
+        cache = _get_cache()
+        adapter = _CacheAdapter(cache) if cache else None
+        out: Dict[int, List[float]] = {}
+        to_query: List[Tuple[int, str]] = []
+
+        hashes = [_hash_text(t) for t in texts]
+        if adapter:
+            got = adapter.get_many([(h, self.model) for h in hashes])
+            for i, h in enumerate(hashes):
+                v = got.get(h)
+                if v and len(v) == self.dim:
+                    out[i] = v
+                else:
+                    to_query.append((i, texts[i]))
+        else:
+            to_query = list(enumerate(texts))
+
+        if to_query:
+            ordered = [t for (_i, t) in to_query]
+            toks = sum(_approx_tokens(t) for t in ordered)
+            self._limiter.account(tokens=toks, requests=1)
+            self._limiter.maybe_sleep()
+
+            backoff = BACKOFF_MIN
+            for attempt in range(MAX_RETRIES):
+                t0 = time.monotonic()
+                try:
+                    with self._client() as cli:
+                        resp = cli.post("/embeddings", json={"model": self.model, "input": ordered})
+                    resp.raise_for_status()
+                    data = resp.json()
+                    vecs_raw = [d["embedding"] for d in data.get("data", [])]
+                    vecs = [self._enforce_dim(v) for v in vecs_raw]
+                    # EMA update
+                    dt_ms = (time.monotonic() - t0) * 1000.0
+                    self._ema_ms = (0.8 * self._ema_ms + 0.2 * dt_ms) if self._ema_ms > 0 else dt_ms
+                    # Cache
+                    if adapter:
+                        tuples = [(_hash_text(t), self.model, self.dim, vec) for t, vec in zip(ordered, vecs)]
+                        adapter.put_many(tuples)
+                    for (i, _), v in zip(to_query, vecs):
+                        out[i] = v
+                    break
+                except Exception as e:
+                    if attempt + 1 >= MAX_RETRIES:
+                        log.error("embed_texts failed after %d attempts: %s", attempt + 1, e)
+                        raise
+                    sleep_s = min(backoff, BACKOFF_MAX)
+                    log.warning("embed_texts attempt %d failed: %s; retrying in %.2fs", attempt + 1, e, sleep_s)
+                    time.sleep(sleep_s)
+                    backoff *= 2.0
+
+        # order-preserving
+        return [out[i] for i in range(len(texts))]
+
+    def debug_snapshot(self) -> Dict[str, float | int | None]:
+        snap = self._limiter.snapshot()
+        return {
+            "rpm_used": snap["rpm_used"],       # type: ignore[index]
+            "tpm_used": snap["tpm_used"],       # type: ignore[index]
+            "rpm_budget": snap["rpm_budget"],   # type: ignore[index]
+            "tpm_budget": snap["tpm_budget"],   # type: ignore[index]
+            "headroom": snap["headroom"],       # type: ignore[index]
+            "cooldown_until": snap["cooldown_until"],  # type: ignore[index]
+            "inflight_without_ttfb": 0,
+            "ema_oneway_ms": round(self._ema_ms, 2),
+        }
+
+
+# ------------------------ Public API ------------------------
+_singleton_fake: Optional[FakeEmbedder] = None
+_singleton_remote: Optional[OpenAIEmbedder] = None
+
+
+def get_embedder(dim_hint: Optional[int] = None, force_local: bool = False):
+    dim = int(dim_hint or DIM_DEFAULT or 1536)
+    if not force_local and ALLOW_REMOTE and OPENAI_API_KEY:
+        global _singleton_remote
+        if _singleton_remote is None or _singleton_remote.dim != dim:
+            _singleton_remote = OpenAIEmbedder(MODEL, dim, OPENAI_TIMEOUT_SECS)
+        return _singleton_remote
+    global _singleton_fake
+    if _singleton_fake is None or _singleton_fake.dim != dim:
+        _singleton_fake = FakeEmbedder(dim)
+    return _singleton_fake
+
+
+def embed_texts(texts: List[str], dim_hint: Optional[int] = None) -> List[List[float]]:
+    """
+    Convenience wrapper used by code_ingest, csv_db_ingest, etc.
+    """
+    emb = get_embedder(dim_hint=dim_hint)
+    return emb.embed_texts(texts)
+
+
+__all__ = ["FakeEmbedder", "OpenAIEmbedder", "get_embedder", "embed_texts", "WINDOW_SEC", "CONCURRENCY"]
*** End Patch
*** Update File: server/app/scripts/ingest_pgvector.py
@@
-# Project:RAG_project_v0.5 Component:scripts.ingest_pgvector Version:v0.8.6
+# Project:RAG_project_v0.5 Component:scripts.ingest_pgvector Version:v0.8.7
 from __future__ import annotations
 import os, sys, json, time, signal, hashlib, logging, traceback, math, threading
 from pathlib import Path
 from typing import List, Dict, Any, Iterable, Tuple, Optional
 
@@
-from ..embeddings import get_embedder
+from ..embeddings import get_embedder
 from ..otel import maybe_span
 from ..reduction import get_reducer
 
-__version__ = "v0.8.6"
+__version__ = "v0.8.7"
@@
-DATA_ROOT     = Path(os.getenv("DATA_ROOT", "/data"))
-INDEX_ROOT    = Path(os.getenv("INDEX_ROOT", "/index"))
+DATA_ROOT     = Path(os.getenv("DATA_ROOT", "/data"))
+INDEX_ROOT    = Path(os.getenv("INDEX_ROOT", "/index"))
 SNAPSHOT_PATH = INDEX_ROOT / "ingest_pgvector.snapshot.json"
 
 EMBED_DIM     = int(os.getenv("EMBED_DIM", "3072"))        # full-fidelity vector size
 ALLOW_REMOTE  = _env_bool("ALLOW_REMOTE_EMBEDDINGS", "0")  # accept 1/true/yes/on
 WATCH         = _env_bool("INGEST_WATCH", "1")
 POLL_SEC      = int(os.getenv("INGEST_POLL_SEC", "300"))
 BATCH_SIZE    = int(os.getenv("EMBED_BATCH", "64"))
 LOG_LEVEL     = os.getenv("LOG_LEVEL", "INFO").upper()
 OPENAI_TO     = int(os.getenv("OPENAI_TIMEOUT_SECS", "30"))  # soft hint for client
-MODEL_HINT    = os.getenv("EMBEDDING_MODEL", os.getenv("MODEL", ""))  # log-only hint
+MODEL_HINT    = os.getenv("EMBEDDING_MODEL", os.getenv("MODEL", ""))  # log-only hint
 
 logging.basicConfig(
     level=getattr(logging, LOG_LEVEL, logging.INFO),
     format="%(asctime)s %(levelname)-7s ingest_pg %(message)s",
     handlers=[logging.StreamHandler(sys.stdout)],
@@
 def upsert_embeddings(rows: List[Dict[str, Any]], embed_dim: int) -> int:
     key = os.getenv("OPENAI_API_KEY", "")
     key_mask = f"{key[:7]}…" if key else "<missing>"
 
     log.info(
-        "Embedding stage: enabled=%s | ALLOW_REMOTE_EMBEDDINGS(raw)=%s | API_KEY=%s | model_hint=%s | dim=%s | batch=%s | timeout=%ss",
+        "Embedding stage: enabled=%s | ALLOW_REMOTE_EMBEDDINGS(raw)=%s | API_KEY=%s | model_hint=%s | dim=%s | batch=%s | timeout=%ss",
         ALLOW_REMOTE, os.getenv("ALLOW_REMOTE_EMBEDDINGS",""),
         ("SET:"+key_mask) if key else "MISSING",
         (os.getenv('EMBEDDING_MODEL') or os.getenv('MODEL') or "<unset>"),
         embed_dim, BATCH_SIZE, OPENAI_TO
     )
@@
     reducer = get_reducer()  # 3072 -> 1536
     total = 0
     batches = max(1, math.ceil(len(rows)/BATCH_SIZE))
 
     with maybe_span("rag.embed"):
         for bi in range(batches):
             if _STOP:
                 break
             lo = bi * BATCH_SIZE
             hi = min((bi+1)*BATCH_SIZE, len(rows))
             chunk = rows[lo:hi]
             ids   = [str(r.get("id")) for r in chunk]
             texts = [(r.get("title") or "") + "\n" + (r.get("body") or "") for r in chunk]
 
-            log.info("Batch %d/%d — requesting embeddings for %d items (ids %s..%s)",
+            log.info("Batch %d/%d — requesting embeddings for %d items (ids %s..%s)",
                      bi+1, batches, len(texts), ids[0] if ids else "-", ids[-1] if ids else "-")
 
             t0 = time.monotonic()
             try:
                 vecs_full = emb.embed_texts(texts)  # 3072
             except Exception as e:
                 log.error("embed_texts() failed at batch %d: %s", bi+1, e)
                 log.debug("Trace:\n%s", traceback.format_exc())
                 break
             t1 = time.monotonic()
             log.info("Batch %d — embeddings OK in %.2fs", bi+1, t1-t0)
 
             try:
                 t2 = time.monotonic()
                 vecs_1536 = [reducer.reduce(v) for v in vecs_full]
                 t3 = time.monotonic()
                 log.info("Batch %d — reduction OK in %.2fs", bi+1, t3-t2)
             except Exception as e:
                 log.error("reduction failed at batch %d: %s", bi+1, e)
                 log.debug("Trace:\n%s", traceback.format_exc())
                 break
 
             with get_conn() as conn:
                 with conn.cursor() as cur:
                     for pid, vf, v1536 in zip(ids, vecs_full, vecs_1536):
                         cur.execute("""
                             INSERT INTO doc_embeddings (id, embedding_full, embedding_1536)
                             VALUES (%s, %s, %s)
                             ON CONFLICT (id) DO UPDATE SET
                               embedding_full = EXCLUDED.embedding_full,
                               embedding_1536 = EXCLUDED.embedding_1536
                         """, (pid, vf, v1536))
                         total += 1
 
             log.info("Batch %d — upserted %d items (cumulative %d)", bi+1, len(ids), total)
 
     log.info("Embedding stage complete — total vectors stored: %d", total)
     return total
@@
 def main():
     log.info(
-        "Start ingest_pgvector %s | DATA_ROOT=%s | INDEX_ROOT=%s | WATCH=%s | POLL=%ss "
-        "| EMBED_DIM=%s | BATCH=%s | OPENAI_TIMEOUT_SECS=%s | MODEL_HINT=%s",
+        "Start ingest_pgvector %s | DATA_ROOT=%s | INDEX_ROOT=%s | WATCH=%s | POLL=%ss "
+        "| EMBED_DIM=%s | BATCH=%s | OPENAI_TIMEOUT_SECS=%s | MODEL_HINT=%s",
         __version__, str(DATA_ROOT), str(INDEX_ROOT), WATCH, POLL_SEC,
         EMBED_DIM, BATCH_SIZE, OPENAI_TO, (MODEL_HINT or "<unset>")
     )
     try:
         if not WATCH:
*** End Patch
*** Update File: server/app/retrieval_hybrid.py
@@
-# Project:RAG_project_v0.5 Component:retrieval_hybrid Version:v0.7.8
+# Project:RAG_project_v0.5 Component:retrieval_hybrid Version:v0.7.9
 from __future__ import annotations
 import os, time
 from typing import List, Dict, Any
 from .db.pg import get_conn
 from .embeddings import get_embedder
 from .reduction import get_reducer
 from .otel import maybe_span
 
 ALPHA = float(os.getenv("HYBRID_ALPHA", "0.5"))
 FULL_DIM = int(os.getenv("EMBED_DIM", "3072"))
+
+# Uniform logging (optional; default INFO)
+import logging
+_LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO").upper()
+_log = logging.getLogger("rag.hybrid")
+if not _log.handlers:
+    logging.basicConfig(level=getattr(logging, _LOG_LEVEL, logging.INFO),
+                        format="%(asctime)s %(levelname)-7s rag.hybrid %(message)s")
+_log.setLevel(getattr(logging, _LOG_LEVEL, logging.INFO))
@@
 def _normalize(scores: List[float]) -> List[float]:
     if not scores:
         return []
     lo, hi = min(scores), max(scores)
     if hi <= lo:
         return [0.0 for _ in scores]
     return [(s - lo) / (hi - lo) for s in scores]
 
 def search_hybrid(q: str, k: int = 20) -> Dict[str, Any]:
     t0 = time.time()
+    _log.debug("hybrid search start q=%r k=%d", q, k)
     lex_hits: List[Dict[str, Any]] = []
     vec_hits: List[Dict[str, Any]] = []
 
     with maybe_span("rag.lex"):
         with get_conn() as conn:
             with conn.cursor() as cur:
                 cur.execute(
@@
     qv_full = None
     with maybe_span("rag.embed"):
         try:
             emb = get_embedder(dim_hint=FULL_DIM)  # 3072
             qv_full = emb.embed_texts([q])[0]
-        except Exception:
+        except Exception as e:
+            _log.warning("hybrid embed error: %s", e)
             qv_full = None
 
     if qv_full is not None:
         with maybe_span("rag.ann"):
             reducer = get_reducer()               # 3072 -> 1536
             qv_1536 = reducer.reduce(qv_full)
             with get_conn() as conn:
                 with conn.cursor() as cur:
                     cur.execute(
                         """
                         SELECT d.id, d.title, d.space, d.url,
                                1.0/(1.0 + (e.embedding_1536 <-> %s)) AS vscore
                         FROM doc_embeddings e
                         JOIN docs d ON d.id = e.id
                         ORDER BY e.embedding_1536 <-> %s
                         LIMIT %s
                         """,
                         (qv_1536, qv_1536, k),
                     )
                     for row in cur.fetchall():
                         vec_hits.append({
                             "id": row[0], "title": row[1], "space": row[2], "url": row[3],
                             "vscore": float(row[4]) if row[4] is not None else None
                         })
@@
     took_ms = int((time.time() - t0) * 1000)
+    _log.debug("hybrid search end took_ms=%d items=%d", took_ms, len(items))
     return {"items": items, "took_ms": took_ms}
*** End Patch
*** Update File: server/app/reduction.py
@@
-# Project:RAG_project_v0.5 Component:reduction Version:v0.7.8
+# Project:RAG_project_v0.5 Component:reduction Version:v0.7.9
 from __future__ import annotations
 import os, hashlib, math
 import numpy as np
 from typing import Iterable, List
 
 _DEF_IN = 3072
 _DEF_OUT = 1536
@@
 class Reducer:
     def __init__(self, in_dim: int = _DEF_IN, out_dim: int = _DEF_OUT, seed: int | None = None):
         self.in_dim = in_dim
         self.out_dim = out_dim
         s = int(os.getenv("RP_SEED", "0")) if seed is None else seed
         self.R = _gaussian_matrix(out_dim, in_dim, s)
 
     def reduce(self, vec: Iterable[float]) -> List[float]:
         x = np.asarray(list(vec), dtype=np.float32)
         if x.size != self.in_dim:
-            raise ValueError(f"expected dim={self_in_dim}, got {x.size}")
+            # Fast-path: if already at out_dim and equals target, return as-is
+            if x.size == self.out_dim:
+                return x.astype(np.float32, copy=False).tolist()
+            raise ValueError(f"expected dim={self.in_dim}, got {x.size}")
         y = self.R @ x
         return y.astype(np.float32).tolist()
 
 _singleton: Reducer | None = None
 
 def get_reducer() -> Reducer:
     global _singleton
     if _singleton is None:
         out_dim = int(os.getenv("REDUCE_TO_DIM", str(_DEF_OUT)))
         _singleton = Reducer(_DEF_IN, out_dim)
     return _singleton
*** End Patch
*** Update File: docker-compose.yml
@@
-# Project:RAG_project_v0.5 Component:compose.unified Version:v0.7.5
+# Project:RAG_project_v0.5 Component:compose.unified Version:v0.7.6
 services:
   pg:
     build:
       context: .
       dockerfile: server/pg/Dockerfile
     container_name: RAG_v0.7.5_rag_pg
     environment:
       POSTGRES_DB: ${POSTGRES_DB:-rag}
       POSTGRES_USER: ${POSTGRES_USER:-rag}
       POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-fabrix}
     ports:
       - "5532:5432"
     volumes:
       - pgdata:/var/lib/postgresql/data
     healthcheck:
       test: ["CMD-SHELL", "pg_isready -U $$POSTGRES_USER -d $$POSTGRES_DB"]
       interval: 10s
       timeout: 3s
       retries: 10
     profiles: ["pg"]
 
   api:
     build: ./server
     container_name: RAG_v0.7.5_api
     environment:
       - DATA_ROOT=/data
       - INDEX_PATH=/index/docs.db
       - INDEX_ROOT=${INDEX_ROOT:-/index}
       - CHUNK_SIZE_BYTES=${CHUNK_SIZE_BYTES:-90000}
       - AUTO_INGEST=${AUTO_INGEST:-0}
       - ALLOW_ORIGINS=${ALLOW_ORIGINS:-*}
       - ADO_ROOT=${ADO_ROOT:-/ado}
       - API_KEY=${API_KEY:-}
-      - ALLOW_REMOTE_EMBEDDINGS=${ALLOW_REMOTE_EMBEDDINGS:-0}
+      - ALLOW_REMOTE_EMBEDDINGS=${ALLOW_REMOTE_EMBEDDINGS:-0}
+      - LOG_LEVEL=${LOG_LEVEL:-INFO}
       - OPENAI_API_KEY=${OPENAI_API_KEY:-}
-      - EMBED_DIM=${EMBED_DIM:-3072}
-      - EMBEDDING_MODEL=${EMBEDDING_MODEL:-text-embedding-3-large}
+      - EMBED_DIM=${EMBED_DIM:-3072}               # Set to 1536 if using text-embedding-3-small
+      - EMBEDDING_MODEL=${EMBEDDING_MODEL:-text-embedding-3-large}
+      - MODEL=${MODEL:-${EMBEDDING_MODEL:-text-embedding-3-large}}
       - EMBED_MAX_TOKENS_PER_REQ=${EMBED_MAX_TOKENS_PER_REQ:-250000}
       - EMBED_MAX_TOKENS_PER_ITEM=${EMBED_MAX_TOKENS_PER_ITEM:-8000}
       - EMBED_TPM_LIMIT=${EMBED_TPM_LIMIT:-1000000}
       - EMBED_RPM_LIMIT=${EMBED_RPM_LIMIT:-5000}
       - EMBED_HEADROOM=${EMBED_HEADROOM:-0.90}
+      - EMBED_CONCURRENCY=${EMBED_CONCURRENCY:-4}
+      - EMBED_WINDOW_SEC=${EMBED_WINDOW_SEC:-60}
       - EMBED_BATCH_SLEEP_MS=${EMBED_BATCH_SLEEP_MS:-0}
       - EMBED_MAX_RETRIES=${EMBED_MAX_RETRIES:-4}
       - EMBED_BACKOFF_MIN_MS=${EMBED_BACKOFF_MIN_MS:-500}
       - EMBED_BACKOFF_MAX_MS=${EMBED_BACKOFF_MAX_MS:-4000}
       - OPENAI_TIMEOUT_SECS=${OPENAI_TIMEOUT_SECS:-30}
       - OTEL_EXPORTER_OTLP_ENDPOINT=${OTEL_EXPORTER_OTLP_ENDPOINT:-}
       - ENABLE_TREE_SITTER=${ENABLE_TREE_SITTER:-0}
       - CODE_ROOT=${CODE_ROOT:-/code}
       - CODE_URL_TEMPLATE=${CODE_URL_TEMPLATE:-}
       # P2 pgvector hookup via standard PG envs (compose-safe)
       - PGHOST=pg
       - PGPORT=5432
       - PGDATABASE=${POSTGRES_DB:-rag}
       - PGUSER=${POSTGRES_USER:-rag}
       - PGPASSWORD=${POSTGRES_PASSWORD:-fabrix}
     ports:
       - "9000:8000"
     volumes:
       - /mnt/disks/data/confgpt_action_server_docker_compose/export_data:/data:ro
       - /mnt/disks/data/confgpt_action_server_docker_compose/index:/index
       - /mnt/disks/data/fetch_ado_items/.ado_cache/8127d161517e897b:/ado:ro
       - ./scripts:/app/scripts:ro
     healthcheck:
       test: ["CMD", "curl", "-fsS", "http://localhost:8000/v1/health"]
       interval: 10s
       timeout: 3s
       retries: 10
     cpus: "2.00"
     mem_reservation: "1g"
     mem_limit: "2g"
     memswap_limit: "2g"
     ulimits:
       nofile:
         soft: 65536
         hard: 65536
     logging:
       driver: json-file
       options: { max-size: "20m", max-file: "5" }
     restart: unless-stopped
 
   ingest:
     build: ./server
     container_name: RAG_v0.7.5_ingest
     command: ["python", "-m", "app.indexer", "--data-root", "/data",
               "--index-path", "/index/docs.db", "--ado-root", "${ADO_ROOT:-/ado}"]
     environment:
       - DATA_ROOT=/data
       - INDEX_PATH=/index/docs.db
       - ADO_ROOT=${ADO_ROOT:-/ado}
       - INDEX_ROOT=${INDEX_ROOT:-/index}
-      - ALLOW_REMOTE_EMBEDDINGS=${ALLOW_REMOTE_EMBEDDINGS:-0}
+      - ALLOW_REMOTE_EMBEDDINGS=${ALLOW_REMOTE_EMBEDDINGS:-0}
+      - LOG_LEVEL=${LOG_LEVEL:-INFO}
     volumes:
       - /mnt/disks/data/confgpt_action_server_docker_compose/export_data:/data:ro
       - /mnt/disks/data/confgpt_action_server_docker_compose/index:/index
       - /mnt/disks/data/fetch_ado_items/.ado_cache/8127d161517e897b:/ado:ro
     profiles: ["tools"]
     restart: on-failure
 
   att_ingest:
     build: ./server
     container_name: RAG_v0.7.5_att_ingest
     command: ["python", "-u", "-m", "app.attachments_ingest", "--spaces", "ALL",
               "--batch", "300"]
     environment:
       - DATA_ROOT=/data
       - INDEX_PATH=/index/docs.db
       - INDEX_ROOT=${INDEX_ROOT:-/index}
       - ADO_ROOT=${ADO_ROOT:-/ado}
       - OCR_ENABLED=${OCR_ENABLED:-0}
       - OCR_PDF_IF_EMPTY=${OCR_PDF_IF_EMPTY:-0}
       - OCR_IMAGES=${OCR_IMAGES:-0}
       - OCR_DPI=${OCR_DPI:-50}
       - OCR_JOBS=${OCR_JOBS:-0}
       - ALLOW_QUERY_FALLBACK=${ALLOW_QUERY_FALLBACK:-1}
-      - ALLOW_REMOTE_EMBEDDINGS=${ALLOW_REMOTE_EMBEDDINGS:-0}
+      - ALLOW_REMOTE_EMBEDDINGS=${ALLOW_REMOTE_EMBEDDINGS:-0}
+      - LOG_LEVEL=${LOG_LEVEL:-INFO}
       - OPENAI_API_KEY=${OPENAI_API_KEY:-}
-      - EMBED_DIM=${EMBED_DIM:-3072}
-      - EMBEDDING_MODEL=${EMBEDDING_MODEL:-text-embedding-3-large}
+      - EMBED_DIM=${EMBED_DIM:-3072}
+      - EMBEDDING_MODEL=${EMBEDDING_MODEL:-text-embedding-3-large}
+      - MODEL=${MODEL:-${EMBEDDING_MODEL:-text-embedding-3-large}}
       - EMBED_MAX_TOKENS_PER_REQ=${EMBED_MAX_TOKENS_PER_REQ:-250000}
       - EMBED_MAX_TOKENS_PER_ITEM=${EMBED_MAX_TOKENS_PER_ITEM:-8000}
       - EMBED_TPM_LIMIT=${EMBED_TPM_LIMIT:-1000000}
       - EMBED_RPM_LIMIT=${EMBED_RPM_LIMIT:-5000}
       - EMBED_HEADROOM=${EMBED_HEADROOM:-0.90}
+      - EMBED_CONCURRENCY=${EMBED_CONCURRENCY:-4}
+      - EMBED_WINDOW_SEC=${EMBED_WINDOW_SEC:-60}
       - EMBED_BATCH_SLEEP_MS=${EMBED_BATCH_SLEEP_MS:-0}
       - EMBED_MAX_RETRIES=${EMBED_MAX_RETRIES:-4}
       - EMBED_BACKOFF_MIN_MS=${EMBED_BACKOFF_MIN_MS:-500}
       - EMBED_BACKOFF_MAX_MS=${EMBED_BACKOFF_MAX_MS:-4000}
       - OPENAI_TIMEOUT_SECS=${OPENAI_TIMEOUT_SECS:-30}
       - ATT_LOG_PATH=/index/attachments_ingest.log
     volumes:
       - /mnt/disks/data/confgpt_action_server_docker_compose/export_data:/data:ro
       - /mnt/disks/data/confgpt_action_server_docker_compose/index:/index
       - /mnt/disks/data/fetch_ado_items/.ado_cache/8127d161517e897b:/ado:ro
     profiles: ["tools"]
     restart: on-failure
 
   code_ingest:
     build: ./server
     container_name: RAG_v0.7.5__code_ingest
     command: ["python", "-u", "-m", "app.code_ingest", "--code-root", "/code"]
     environment:
       - INDEX_ROOT=${INDEX_ROOT:-/index}
       - CODE_ROOT=/code
       - CODE_SPACE=${CODE_SPACE:-CODE}
       - CODE_LOG_PATH=/index/code_ingest.log
       - CODE_MAX_FILE_BYTES=${CODE_MAX_FILE_BYTES:-3145728}
       - CODE_CHUNK_LINES=${CODE_CHUNK_LINES:-180}
       - CODE_CHUNK_OVERLAP=${CODE_CHUNK_OVERLAP:-40}
       - CODE_BATCH_EMBED_SIZE=${CODE_BATCH_EMBED_SIZE:-256}
       - CODE_FLUSH_EVERY_FILES=${CODE_FLUSH_EVERY_FILES:-50}
       - CODE_TARGET_TOKENS=${CODE_TARGET_TOKENS:-800}
       - CODE_OVERLAP_TOKENS=${CODE_OVERLAP_TOKENS:-120}
       - CODE_ALLOW_PATH_RE=${CODE_ALLOW_PATH_RE:-}
       - CODE_DENY_PATH_RE=${CODE_DENY_PATH_RE:-}
       - CODE_MAX_DEPTH=${CODE_MAX_DEPTH:-0}
       - CODE_MAX_FILES=${CODE_MAX_FILES:-0}
       - CODE_READ_WORKERS=${CODE_READ_WORKERS:-4}
       - CODE_CACHE_PATH=${CODE_CACHE_PATH:-/index/code_ingest_cache.sqlite}
       - CODE_PROGRESS_PATH=${CODE_PROGRESS_PATH:-/index/code_ingest.progress.json}
       - CODE_REPO_NAME=${CODE_REPO_NAME:-}
       - CODE_BUILD_XREF=${CODE_BUILD_XREF:-0}
-      - ALLOW_REMOTE_EMBEDDINGS=${ALLOW_REMOTE_EMBEDDINGS:-0}
+      - ALLOW_REMOTE_EMBEDDINGS=${ALLOW_REMOTE_EMBEDDINGS:-0}
+      - LOG_LEVEL=${LOG_LEVEL:-INFO}
       - OPENAI_API_KEY=${OPENAI_API_KEY:-}
-      - EMBED_DIM=${EMBED_DIM:-3072}
-      - EMBEDDING_MODEL=${EMBEDDING_MODEL:-text-embedding-3-large}
+      - EMBED_DIM=${EMBED_DIM:-3072}
+      - EMBEDDING_MODEL=${EMBEDDING_MODEL:-text-embedding-3-large}
+      - MODEL=${MODEL:-${EMBEDDING_MODEL:-text-embedding-3-large}}
       - EMBED_MAX_TOKENS_PER_REQ=${EMBED_MAX_TOKENS_PER_REQ:-250000}
       - EMBED_MAX_TOKENS_PER_ITEM=${EMBED_MAX_TOKENS_PER_ITEM:-8000}
       - EMBED_TPM_LIMIT=${EMBED_TPM_LIMIT:-1000000}
       - EMBED_RPM_LIMIT=${EMBED_RPM_LIMIT:-5000}
       - EMBED_HEADROOM=${EMBED_HEADROOM:-0.90}
+      - EMBED_CONCURRENCY=${EMBED_CONCURRENCY:-4}
+      - EMBED_WINDOW_SEC=${EMBED_WINDOW_SEC:-60}
       - EMBED_BATCH_SLEEP_MS=${EMBED_BATCH_SLEEP_MS:-0}
       - EMBED_MAX_RETRIES=${EMBED_MAX_RETRIES:-4}
       - EMBED_BACKOFF_MIN_MS=${EMBED_BACKOFF_MIN_MS:-500}
       - EMBED_BACKOFF_MAX_MS=${EMBED_BACKOFF_MAX_MS:-4000}
       - OPENAI_TIMEOUT_SECS=${OPENAI_TIMEOUT_SECS:-30}
     volumes:
       - ${CODE_HOST_PATH:-/mnt/disks/data/source-code}:/code:ro
       - ./index:/index
     profiles: ["tools"]
     restart: on-failure
 
   ingest_pg:
     build: ./server
     container_name: RAG_v0.7.5_ingest_pg
     command: ["python", "-m", "app.scripts.ingest_pgvector"]
     environment:
       - DATA_ROOT=/data
       - PGHOST=pg
       - PGPORT=5432
       - PGDATABASE=${POSTGRES_DB:-rag}
       - PGUSER=${POSTGRES_USER:-rag}
       - PGPASSWORD=${POSTGRES_PASSWORD:-fabrix}
-      - EMBED_DIM=${EMBED_DIM:-3072}
-      - ALLOW_REMOTE_EMBEDDINGS=${ALLOW_REMOTE_EMBEDDINGS:-0}
+      - LOG_LEVEL=${LOG_LEVEL:-INFO}
+      - EMBED_DIM=${EMBED_DIM:-3072}               # Set to 1536 if using text-embedding-3-small
+      - ALLOW_REMOTE_EMBEDDINGS=${ALLOW_REMOTE_EMBEDDINGS:-0}
+      - EMBED_BATCH=${EMBED_BATCH:-64}             # Lower to 8 if you see timeouts
+      - INGEST_WATCH=${INGEST_WATCH:-1}
+      - INGEST_POLL_SEC=${INGEST_POLL_SEC:-300}
+      - OPENAI_TIMEOUT_SECS=${OPENAI_TIMEOUT_SECS:-30}
+      - EMBEDDING_MODEL=${EMBEDDING_MODEL:-text-embedding-3-large}
+      - MODEL=${MODEL:-${EMBEDDING_MODEL:-text-embedding-3-large}}
       - OPENAI_API_KEY=${OPENAI_API_KEY:-}
     volumes:
-      - /mnt/disks/data/confgpt_action_server_docker_compose/export_data:/data:ro
+      - /mnt/disks/data/confgpt_action_server_docker_compose/export_data:/data:ro
+      - /mnt/disks/data/confgpt_action_server_docker_compose/index:/index
     depends_on:
       pg:
         condition: service_healthy
     profiles: ["pg", "tools"]
     restart: on-failure
 
 volumes:
   pgdata:
*** End Patch
